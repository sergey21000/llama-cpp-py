name: Tests

on: workflow_dispatch

permissions:
  contents: read

env:
  PYTHONUTF8: 1
  LLM_MODEL_DIR: models
  LLM_MODEL_REPO: bartowski/Qwen_Qwen3-0.6B-GGUF
  LLM_MODEL_FILE: Qwen_Qwen3-0.6B-Q4_K_M.gguf
  LLAMA_ARG_CTX_SIZE: 4096
  LLAMA_ARG_NO_WEBUI: 1
  LLAMA_ARG_JINJA: 1

jobs:
  download-model:
    name: Download model
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v5

      - name: Install uv
        uses: astral-sh/setup-uv@v7
        with:
          enable-cache: true

      - name: Set up Python
        run: |
          uv python install 3.12
          uv venv
          
      - name: Install huggingface-hub
        run: uv pip install huggingface_hub[cli]

      - name: Download models
        run: |
          uv run hf download ${{ env.LLM_MODEL_REPO }} ${{ env.LLM_MODEL_FILE }} \
          --local-dir ${{ env.LLM_MODEL_DIR }}
          
      - name: Upload model artifact
        uses: actions/upload-artifact@v4
        with:
          name: model
          path: ${{ env.LLM_MODEL_DIR }}
          overwrite: true
          retention-days: 1

  test:
    name: Test
    needs: download-model    
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]
        python-version: ["3.12"]
    runs-on: ${{ matrix.os }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v5
      - name: Install uv
        uses: astral-sh/setup-uv@v7
        with:
          enable-cache: true

      - name: Set up Python
        run: |
          uv python install ${{ matrix.python-version }}
          uv venv

      - name: Install dependencies
        run: |
          uv pip install .[test]

      - name: Download models artifact
        uses: actions/download-artifact@v5
        with:
          name: model
          path: ${{ env.LLM_MODEL_DIR }}
      
      - name: Check models exists (Linux/macOS)
        if: runner.os != 'Windows'
        run: |
          echo "LLM model dir:"
          ls -lh ${{ env.LLM_MODEL_DIR }}

      - name: Check models exists (Windows)
        if: runner.os == 'Windows'
        run: |
          echo "LLM model dir:"
          dir ${{ env.LLM_MODEL_DIR }}
        shell: pwsh
        
      - name: Test with pytest
        env:
          LLAMA_ARG_MODEL: ${{ env.LLM_MODEL_DIR }}/${{ env.LLM_MODEL_FILE }}
        run: |
          uv run python -m pytest -vs
