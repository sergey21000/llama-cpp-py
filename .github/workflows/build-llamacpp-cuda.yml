name: Build llama.cpp CUDA (linux x64)

on:
  workflow_dispatch:

permissions:
  contents: write

env:
  CMAKE_ARGS: >
    -DLLAMA_BUILD_EXAMPLES=OFF
    -DLLAMA_BUILD_TESTS=OFF
    -DLLAMA_BUILD_TOOLS=ON
    -DLLAMA_BUILD_SERVER=ON
    -DGGML_RPC=ON
    -DGGML_CUDA=ON
    -DCMAKE_CUDA_ARCHITECTURES=70

jobs:
  build-cuda-linux-x64:
    runs-on: ubuntu-22.04

    steps:
      - name: Checkout llama.cpp
        uses: actions/checkout@v6
        with:
          repository: ggml-org/llama.cpp
          ref: master
          fetch-depth: 0

      - name: ccache
        uses: ggml-org/ccache-action@v1.2.16
        with:
          key: ubuntu-cuda-cmake-x64
          evict-old-files: 1d

      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y \
            build-essential \
            libssl-dev \
            nvidia-cuda-toolkit

      - name: Build (CUDA)
        run: |
          cmake -B build \
            -DCMAKE_BUILD_TYPE=Release \
            -DCMAKE_INSTALL_RPATH='$ORIGIN' \
            -DCMAKE_BUILD_WITH_INSTALL_RPATH=ON \
            -DGGML_BACKEND_DL=ON \
            -DGGML_NATIVE=OFF \
            -DGGML_CPU_ALL_VARIANTS=ON \
            -DLLAMA_FATAL_WARNINGS=ON \
            $CMAKE_ARGS

          cmake --build build -j $(nproc)

      - name: Determine tag name
        id: tag
        uses: ./.github/actions/get-tag-name

      - name: Pack artifacts
        run: |
          cp LICENSE build/bin/
          tar -czvf llama-${{ steps.tag.outputs.name }}-bin-ubuntu-cuda-x64.tar.gz \
            --transform "s,./,llama-${{ steps.tag.outputs.name }}/," \
            -C build/bin .

      - name: Upload artifacts
        uses: actions/upload-artifact@v6
        with:
          name: llama-bin-ubuntu-cuda-x64
          path: llama-${{ steps.tag.outputs.name }}-bin-ubuntu-cuda-x64.tar.gz
