# ==============================================================
# llama.cpp Environment Variables
# https://github.com/ggml-org/llama.cpp/blob/master/tools/server/README.md
# ==============================================================

# ==============================================================
# llama.cpp choosing an LLM model

# option 1
LLAMA_ARG_MODEL_URL=https://huggingface.co/bartowski/Qwen_Qwen3-0.6B-GGUF/resolve/main/Qwen_Qwen3-0.6B-Q4_K_M.gguf

# option 2
# LLAMA_ARG_MODEL=D:/models/Qwen_Qwen3-0.6B-Q4_K_M.gguf

# option 3
# LLAMA_ARG_HF_REPO=bartowski/Qwen_Qwen3-0.6B-GGUF:q4_k_m

# option 4
# LLAMA_ARG_HF_REPO=bartowski/Qwen_Qwen3-0.6B-GGUF
# LLAMA_ARG_HF_FILE=Qwen_Qwen3-0.6B-Q4_K_M.gguf

# ==============================================================
# llama.cpp choosing an VLM model

# option 1
# gemma-3-4b
# LLAMA_ARG_MODEL_URL=https://huggingface.co/bartowski/google_gemma-3-4b-it-GGUF/resolve/main/google_gemma-3-4b-it-Q4_K_M.gguf
# LLAMA_ARG_MMPROJ_URL=https://huggingface.co/bartowski/google_gemma-3-4b-it-GGUF/resolve/main/mmproj-google_gemma-3-4b-it-f16.gguf

# qwen3-vl 4B
# LLAMA_ARG_MODEL_URL=https://huggingface.co/Qwen/Qwen3-VL-4B-Instruct-GGUF/resolve/main/Qwen3VL-4B-Instruct-Q4_K_M.gguf
# LLAMA_ARG_MMPROJ_URL=https://huggingface.co/Qwen/Qwen3-VL-4B-Instruct-GGUF/resolve/main/mmproj-Qwen3VL-4B-Instruct-Q8_0.gguf

# qwen3-vl 2B
# LLAMA_ARG_MODEL_URL=https://huggingface.co/Qwen/Qwen3-VL-2B-Instruct-GGUF/resolve/main/Qwen3VL-2B-Instruct-Q4_K_M.gguf
# LLAMA_ARG_MMPROJ_URL=https://huggingface.co/Qwen/Qwen3-VL-2B-Instruct-GGUF/resolve/main/mmproj-Qwen3VL-2B-Instruct-Q8_0.gguf

# option 2
# LLAMA_ARG_MODEL=D:/models/bartowski_google_gemma-3-4b-it-GGUF_google_gemma-3-4b-it-Q4_K_M.gguf
# LLAMA_ARG_MMPROJ=D:/models/mmproj-google_gemma-3-4b-it-f16.gguf

# option 3 (mmproj is also downloaded automatically if available)
# LLAMA_ARG_HF_REPO=Qwen/Qwen3-VL-2B-Instruct-GGUF:q4_k_m

# disabling mmproj model loading (default: 1)
# LLAMA_ARG_MMPROJ_AUTO=0

# ==============================================================
# llama.cpp settings

LLAMA_ARG_JINJA=1  # default 0
LLAMA_ARG_CTX_SIZE=4096
LLAMA_ARG_NO_WEBUI=1
LLAMA_LOG_VERBOSITY=0
LLAMA_ARG_N_PARALLEL=1
LLAMA_ARG_N_GPU_LAYERS=-1
# LLAMA_CACHE="llama_cache"
# HF_TOKEN=""

# ==============================================================
# llama.cpp endpoint

LLAMA_ARG_PORT=8080
LLAMA_ARG_HOST=127.0.0.1
