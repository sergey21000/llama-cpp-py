# llama.cpp common params
# https://github.com/ggml-org/llama.cpp/blob/master/tools/server/README.md#usage

# LLAMA_ARG_HF_REPO=bartowski/Qwen_Qwen3-0.6B-GGUF
# LLAMA_ARG_HF_FILE=Qwen_Qwen3-0.6B-Q4_K_M.gguf
# LLAMA_ARG_MODEL_URL=https://huggingface.co/bartowski/Qwen_Qwen3-0.6B-GGUF/resolve/main/Qwen_Qwen3-0.6B-Q4_K_M.gguf
# LLAMA_ARG_MODEL=Qwen_Qwen3-0.6B-Q4_K_M.gguf
LLAMA_ARG_MODEL=D:/models/Qwen_Qwen3-0.6B-Q4_K_M.gguf

LLAMA_ARG_CTX_SIZE=4096
LLAMA_ARG_NO_WEBUI=1
LLAMA_LOG_VERBOSITY=0
LLAMA_ARG_N_PARALLEL=1
LLAMA_ARG_N_GPU_LAYERS=0

# llama.cpp endpoint
LLAMA_ARG_PORT=8080
LLAMA_ARG_HOST=127.0.0.1

# llama.cpp predict params
LLAMA_ARG_N_PREDICT=-1

LLAMA_ARG_JINJA=1  # default 0
