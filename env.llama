# ==============================================================
# llama.cpp Environment Variables
# https://github.com/ggml-org/llama.cpp/blob/master/tools/server/README.md
# ==============================================================

# ==============================================================
# llama.cpp choosing an LLM model

# option 1
LLAMA_ARG_MODEL_URL=https://huggingface.co/bartowski/Qwen_Qwen3-0.6B-GGUF/resolve/main/Qwen_Qwen3-0.6B-Q4_K_M.gguf
# LLAMA_ARG_MODEL_URL=https://huggingface.co/bartowski/Qwen_Qwen3-0.6B-GGUF/resolve/main/Qwen_Qwen3-0.6B-Q4_K_M.gguf
# LLAMA_ARG_MODEL_URL=https://huggingface.co/bartowski/Qwen_Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Instruct-2507-Q4_K_M.gguf
# LLAMA_ARG_MODEL_URL=https://huggingface.co/bartowski/Qwen_Qwen3-1.7B-GGUF/resolve/main/Qwen_Qwen3-1.7B-Q4_K_M.gguf
# LLAMA_ARG_MODEL_URL=https://huggingface.co/bartowski/Qwen_Qwen3-1.7B-GGUF/resolve/main/Qwen_Qwen3-1.7B-Q8_0.gguf
# LLAMA_ARG_MODEL_URL=https://huggingface.co/bartowski/google_gemma-3-1b-it-GGUF/resolve/main/google_gemma-3-1b-it-Q8_0.gguf
# LLAMA_ARG_MODEL_URL=https://huggingface.co/ggml-org/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-Q4_K_M.gguf

# option 2
# LLAMA_ARG_MODEL=D:/models/Qwen_Qwen3-0.6B-Q4_K_M.gguf

# option 3
# LLAMA_ARG_HF_REPO=bartowski/Qwen_Qwen3-0.6B-GGUF:q4_k_m
# LLAMA_ARG_HF_REPO=bartowski/Qwen_Qwen3-1.7B-GGUF:q8_0
# LLAMA_ARG_HF_REPO=bartowski/Qwen_Qwen3-4B-Instruct-2507-GGUF:q4_k_m
# LLAMA_ARG_HF_REPO=bartowski/google_gemma-3-1b-it-GGUF:q8_0
# LLAMA_ARG_HF_REPO=bartowski/google_gemma-3-4b-it-GGUF:q4_k_m

# option 4
# LLAMA_ARG_HF_REPO=bartowski/Qwen_Qwen3-0.6B-GGUF
# LLAMA_ARG_HF_FILE=Qwen_Qwen3-0.6B-Q4_K_M.gguf

# ==============================================================
# llama.cpp choosing an VLM model

# option 1
# gemma-3-4b
# LLAMA_ARG_MODEL_URL=https://huggingface.co/bartowski/google_gemma-3-4b-it-GGUF/resolve/main/google_gemma-3-4b-it-Q4_K_M.gguf
# LLAMA_ARG_MMPROJ_URL=https://huggingface.co/bartowski/google_gemma-3-4b-it-GGUF/resolve/main/mmproj-google_gemma-3-4b-it-f16.gguf

# qwen3-vl 4B
# LLAMA_ARG_MODEL_URL=https://huggingface.co/Qwen/Qwen3-VL-4B-Instruct-GGUF/resolve/main/Qwen3VL-4B-Instruct-Q4_K_M.gguf
# LLAMA_ARG_MMPROJ_URL=https://huggingface.co/Qwen/Qwen3-VL-4B-Instruct-GGUF/resolve/main/mmproj-Qwen3VL-4B-Instruct-Q8_0.gguf

# qwen3-vl 2B
# LLAMA_ARG_MODEL_URL=https://huggingface.co/Qwen/Qwen3-VL-2B-Instruct-GGUF/resolve/main/Qwen3VL-2B-Instruct-Q4_K_M.gguf
# LLAMA_ARG_MMPROJ_URL=https://huggingface.co/Qwen/Qwen3-VL-2B-Instruct-GGUF/resolve/main/mmproj-Qwen3VL-2B-Instruct-Q8_0.gguf

# option 2
# LLAMA_ARG_MODEL=D:/models/bartowski_google_gemma-3-4b-it-GGUF_google_gemma-3-4b-it-Q4_K_M.gguf
# LLAMA_ARG_MMPROJ=D:/models/mmproj-google_gemma-3-4b-it-f16.gguf

# option 3 (mmproj is also downloaded automatically if available)
# LLAMA_ARG_HF_REPO=Qwen/Qwen3-VL-2B-Instruct-GGUF:q4_k_m
# LLAMA_ARG_HF_REPO=Qwen/Qwen3-VL-4B-Instruct-GGUF:q4_k_m
# LLAMA_ARG_HF_REPO=bartowski/google_gemma-3-4b-it-GGUF:q4_k_m

# disabling mmproj model loading (default: 1)
# LLAMA_ARG_MMPROJ_AUTO=0

# ==============================================================
# llama.cpp settings

LLAMA_ARG_JINJA=1  # default 0
LLAMA_ARG_CTX_SIZE=4096
LLAMA_ARG_N_PARALLEL=1
LLAMA_ARG_N_GPU_LAYERS=-1
LLAMA_LOG_VERBOSITY=1
LLAMA_LOG_COLORS=on
LLAMA_ARG_NO_WEBUI=1

# LLAMA_ARG_MODELS_DIR=llama_cache
# LLAMA_ARG_MODELS_MAX=2
# LLAMA_CACHE=llama_cache

# HF_TOKEN=""

# ==============================================================
# llama.cpp endpoint

LLAMA_ARG_PORT=8081
LLAMA_ARG_HOST=0.0.0.0


# ==============================================================
# llama-cpp-py Environment Variables
# https://github.com/sergey21000/llama-cpp-py#enviroment-variables
# ==============================================================

LLAMACPP_SERVER_TIMEOUT_WAIT=900
# LLAMACPP_RELEASE_TAG=b7806
# LLAMACPP_RELEASE_ZIP_URL=https://github.com/ggml-org/llama.cpp/releases/download/b7806/llama-b7806-bin-win-cuda-13.1-x64.zip
# LLAMACPP_DIR="/content/llama.cpp/build/bin"
# LLAMACPP_LOG_LEVEL=DEBUG
